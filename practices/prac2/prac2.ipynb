{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2aA14kYCYSwo"
   },
   "source": [
    "## Динамическое программирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "roQ4hAstjVce"
   },
   "source": [
    "Рассмотрим алгоритм итерации по оценкам состояния $V$ (Value Iteration):\n",
    "$$\n",
    "V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]\n",
    "$$\n",
    "На основе оценки $V_i$ можно посчитать функцию оценки $Q_i$ действия $a$ в состоянии $s$:\n",
    "$$\n",
    "Q_i(s, a) = \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]\n",
    "$$\n",
    "$$\n",
    "V_{(i+1)}(s) = \\max_a Q_i(s,a)\n",
    "$$\n",
    "\n",
    "Зададим напрямую модель MDP с картинки:\n",
    "<img src=\"https://raw.githubusercontent.com/Tviskaron/mipt/master/2019/RL/02/mdp.png\" caption=\"Марковский процесс принятия решений\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "l6LwgNvgYXIP"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    COLAB = True\n",
    "except ModuleNotFoundError:\n",
    "    COLAB = False\n",
    "    pass\n",
    "\n",
    "if COLAB:\n",
    "    !wget https://raw.githubusercontent.com/Tviskaron/mipt/master/2019/RL/02/mdp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OpKyGJEJYYDn"
   },
   "outputs": [],
   "source": [
    "transition_probs = {\n",
    "  's0':{\n",
    "    'a0': {'s0': 0.5, 's2': 0.5},\n",
    "    'a1': {'s2': 1}\n",
    "  },\n",
    "  's1':{\n",
    "    'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "    'a1': {'s1': 0.95, 's2': 0.05}\n",
    "  },\n",
    "  's2':{\n",
    "    'a0': {'s0': 0.4, 's2': 0.6},\n",
    "    'a1': {'s0': 0.3, 's1': 0.3, 's2':0.4}\n",
    "  }\n",
    "}\n",
    "rewards = {\n",
    "  's1': {'a0': {'s0': +5}},\n",
    "  's2': {'a1': {'s0': -1}}\n",
    "}\n",
    "\n",
    "from mdp import MDP\n",
    "import numpy as np\n",
    "mdp = MDP(transition_probs, rewards, initial_state='s0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVSC6KXuYcsh"
   },
   "source": [
    "Теперь мы можем использовать это MDP, как и любое другое gym окружение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PzLyFJ4iYfro"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial state = s0\n",
      "next_state =s2, reward = 0.0, done = False\n"
     ]
    }
   ],
   "source": [
    "state = mdp.reset()\n",
    "print('initial state =', state)\n",
    "next_state, reward, done, info = mdp.step('a1')\n",
    "print(f'next_state ={next_state}, reward = {reward}, done = {done}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgRdVPJlYjZ4"
   },
   "source": [
    ":Также, помимо стандартных методов, есть дополнительные, которые пригодятся нам для реализации метода итерации по полезностям."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4zK1xXedYn21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_states = ('s0', 's1', 's2')\n",
      "possible_actions('s1') =  ('a0', 'a1')\n",
      "next_states('s1', 'a0') =  {'s0': 0.7, 's1': 0.1, 's2': 0.2}\n",
      "reward('s1', 'a0', 's0') =  5\n",
      "transition_prob('s1', 'a0', 's0') =  0.7\n"
     ]
    }
   ],
   "source": [
    "print(\"all_states =\", mdp.get_all_states())\n",
    "print(\"possible_actions('s1') = \", mdp.get_possible_actions('s1'))\n",
    "print(\"next_states('s1', 'a0') = \", mdp.get_next_states('s1', 'a0'))\n",
    "print(\"reward('s1', 'a0', 's0') = \",mdp.get_reward('s1', 'a0', 's0'))\n",
    "print(\"transition_prob('s1', 'a0', 's0') = \",\n",
    "      mdp.get_transition_prob('s1', 'a0', 's0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Oe_RzZtYq11"
   },
   "source": [
    "### Задание 1\n",
    "\n",
    "Теперь реализуем алгоритм итерации по полезностям, чтобы решить этот вручную заданный MDP. Псевдокод алгоритма:\n",
    "\n",
    "---\n",
    "\n",
    "`1.` Инициализируем $V^{(0)}(s)=0$, для всех $s$\n",
    "\n",
    "`2.` For $i=0, 1, 2, \\dots$\n",
    "\n",
    "`3.` $ \\quad V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$, для всех $s$\n",
    "\n",
    "---\n",
    "\n",
    "Вначале вычисляем оценку состояния-действия:\n",
    "$$Q_i(s, a) = \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1aA0DQccjody"
   },
   "source": [
    "### 1 балл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Qt0o0MokYv0F"
   },
   "outputs": [],
   "source": [
    "def get_action_value(\n",
    "    mdp, state_values, state, action, gamma\n",
    "):\n",
    "    \"\"\" Вычисляем Q(s,a) по формуле выше \"\"\"\n",
    "\n",
    "    Q = 0\n",
    "    for next_state, prob in mdp.get_next_states(state, action).items():\n",
    "        r = mdp.get_reward(state, action, next_state)\n",
    "        Q += prob * (r + gamma * state_values[next_state])\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "x06WscSIYysp"
   },
   "outputs": [],
   "source": [
    "test_Vs = {s: i for i, s in enumerate(sorted(mdp.get_all_states()))}\n",
    "assert np.isclose(get_action_value(mdp, test_Vs, 's2', 'a1', 0.9), 0.69)\n",
    "assert np.isclose(get_action_value(mdp, test_Vs, 's1', 'a0', 0.9), 3.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87q6GhsMY19h"
   },
   "source": [
    "Теперь оцениваем полезность самого состояния, для этого мы можем использовать предыдущий метод:\n",
    "\n",
    "$$V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = \\max_a Q_i(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2O3QFuoVj1iZ"
   },
   "source": [
    "### 1 балл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "hFqCuRaBY5J_"
   },
   "outputs": [],
   "source": [
    "def get_new_state_value(mdp, state_values, state, gamma):\n",
    "    \"\"\" Считаем следующее V(s) по формуле выше.\"\"\"\n",
    "    if mdp.is_terminal(state):\n",
    "        return 0\n",
    "\n",
    "    V = -np.inf\n",
    "    for action in mdp.get_possible_actions(state):\n",
    "        Q = get_action_value(mdp, state_values, state, action, gamma)\n",
    "        V = max(V, Q)\n",
    "\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "lPUyRzQOY8PP"
   },
   "outputs": [],
   "source": [
    "test_Vs_copy = dict(test_Vs)\n",
    "assert np.isclose(get_new_state_value(mdp, test_Vs, 's0', 0.9), 1.8)\n",
    "assert np.isclose(get_new_state_value(mdp, test_Vs, 's2', 0.9), 1.08)\n",
    "assert np.isclose(get_new_state_value(mdp, {'s0': -1e10, 's1': 0, 's2': -2e10}, 's0', 0.9), -13500000000.0), \\\n",
    "   \"Убедитесь, что вы правильно обрабатываете отрицательные значения Q произвольной величины.\"\n",
    "assert test_Vs == test_Vs_copy, \"Убедитесь, что вы не изменяете state_values в функции get_new_state_value\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Od-SBiPKY_Q3"
   },
   "source": [
    "Теперь создаем основной цикл итерационного оценки полезности состояний с критерием остановки, который проверяет насколько изменились полезности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-0-PlvmkF7P"
   },
   "source": [
    "### 1 балл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "uUwb5JCDZDD4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter    0 | diff: 3.50000 | V(start): 0.000 \n",
      "iter    1 | diff: 0.64500 | V(start): 0.000 \n",
      "iter    2 | diff: 0.58050 | V(start): 0.581 \n",
      "iter    3 | diff: 0.43582 | V(start): 0.866 \n",
      "iter    4 | diff: 0.30634 | V(start): 1.145 \n",
      "iter    5 | diff: 0.27571 | V(start): 1.421 \n",
      "iter    6 | diff: 0.24347 | V(start): 1.655 \n",
      "iter    7 | diff: 0.21419 | V(start): 1.868 \n",
      "iter    8 | diff: 0.19277 | V(start): 2.061 \n",
      "iter    9 | diff: 0.17327 | V(start): 2.233 \n",
      "iter   10 | diff: 0.15569 | V(start): 2.389 \n",
      "iter   11 | diff: 0.14012 | V(start): 2.529 \n",
      "iter   12 | diff: 0.12610 | V(start): 2.655 \n",
      "iter   13 | diff: 0.11348 | V(start): 2.769 \n",
      "iter   14 | diff: 0.10213 | V(start): 2.871 \n",
      "iter   15 | diff: 0.09192 | V(start): 2.963 \n",
      "iter   16 | diff: 0.08272 | V(start): 3.045 \n",
      "iter   17 | diff: 0.07445 | V(start): 3.120 \n",
      "iter   18 | diff: 0.06701 | V(start): 3.187 \n",
      "iter   19 | diff: 0.06031 | V(start): 3.247 \n",
      "iter   20 | diff: 0.05428 | V(start): 3.301 \n",
      "iter   21 | diff: 0.04885 | V(start): 3.350 \n",
      "iter   22 | diff: 0.04396 | V(start): 3.394 \n",
      "iter   23 | diff: 0.03957 | V(start): 3.434 \n",
      "iter   24 | diff: 0.03561 | V(start): 3.469 \n",
      "iter   25 | diff: 0.03205 | V(start): 3.502 \n",
      "iter   26 | diff: 0.02884 | V(start): 3.530 \n",
      "iter   27 | diff: 0.02596 | V(start): 3.556 \n",
      "iter   28 | diff: 0.02336 | V(start): 3.580 \n",
      "iter   29 | diff: 0.02103 | V(start): 3.601 \n",
      "iter   30 | diff: 0.01892 | V(start): 3.620 \n",
      "iter   31 | diff: 0.01703 | V(start): 3.637 \n",
      "iter   32 | diff: 0.01533 | V(start): 3.652 \n",
      "iter   33 | diff: 0.01380 | V(start): 3.666 \n",
      "iter   34 | diff: 0.01242 | V(start): 3.678 \n",
      "iter   35 | diff: 0.01117 | V(start): 3.689 \n",
      "iter   36 | diff: 0.01006 | V(start): 3.699 \n",
      "iter   37 | diff: 0.00905 | V(start): 3.708 \n",
      "iter   38 | diff: 0.00815 | V(start): 3.717 \n",
      "iter   39 | diff: 0.00733 | V(start): 3.724 \n",
      "iter   40 | diff: 0.00660 | V(start): 3.731 \n",
      "iter   41 | diff: 0.00594 | V(start): 3.736 \n",
      "iter   42 | diff: 0.00534 | V(start): 3.742 \n",
      "iter   43 | diff: 0.00481 | V(start): 3.747 \n",
      "iter   44 | diff: 0.00433 | V(start): 3.751 \n",
      "iter   45 | diff: 0.00390 | V(start): 3.755 \n",
      "iter   46 | diff: 0.00351 | V(start): 3.758 \n",
      "iter   47 | diff: 0.00316 | V(start): 3.762 \n",
      "iter   48 | diff: 0.00284 | V(start): 3.764 \n",
      "iter   49 | diff: 0.00256 | V(start): 3.767 \n",
      "iter   50 | diff: 0.00230 | V(start): 3.769 \n",
      "iter   51 | diff: 0.00207 | V(start): 3.771 \n",
      "iter   52 | diff: 0.00186 | V(start): 3.773 \n",
      "iter   53 | diff: 0.00168 | V(start): 3.775 \n",
      "iter   54 | diff: 0.00151 | V(start): 3.776 \n",
      "iter   55 | diff: 0.00136 | V(start): 3.778 \n",
      "iter   56 | diff: 0.00122 | V(start): 3.779 \n",
      "iter   57 | diff: 0.00110 | V(start): 3.780 \n",
      "iter   58 | diff: 0.00099 | V(start): 3.781 \n",
      "Принято! Алгоритм сходится!\n"
     ]
    }
   ],
   "source": [
    "def value_iteration(\n",
    "    mdp, state_values=None,\n",
    "    gamma = 0.9, num_iter = 1000, min_difference = 1e-5\n",
    "):\n",
    "    \"\"\" выполняет num_iter шагов итерации по значениям\"\"\"\n",
    "    # инициализируем V(s)\n",
    "    state_values = state_values or \\\n",
    "    {s : 0 for s in mdp.get_all_states()}\n",
    "\n",
    "    for i in range(num_iter):\n",
    "        new_state_values = {}\n",
    "        for state in mdp.get_all_states():\n",
    "            new_state_values[state] = get_new_state_value(mdp, state_values, state, gamma)\n",
    "\n",
    "        assert isinstance(new_state_values, dict)\n",
    "\n",
    "        # Считаем разницу\n",
    "        diff = max(\n",
    "            abs(new_state_values[s] - state_values[s])\n",
    "            for s in mdp.get_all_states()\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"iter {i:4} | diff: {diff:6.5f} \"\n",
    "            f\"| V(start): {new_state_values[mdp._initial_state]:.3f} \"\n",
    "        )\n",
    "\n",
    "        state_values = new_state_values\n",
    "        if diff < min_difference:\n",
    "            print(\"Принято! Алгоритм сходится!\")\n",
    "            break\n",
    "\n",
    "    return state_values\n",
    "\n",
    "state_values = value_iteration(\n",
    "    mdp, num_iter = 100, min_difference = 0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ZKomkPSrZGlZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final state values: {'s0': 3.7810348735476405, 's1': 7.294006423867229, 's2': 4.202140275227048}\n"
     ]
    }
   ],
   "source": [
    "print(\"Final state values:\", state_values)\n",
    "\n",
    "assert abs(state_values['s0'] - 3.781) < 0.01\n",
    "assert abs(state_values['s1'] - 7.294) < 0.01\n",
    "assert abs(state_values['s2'] - 4.202) < 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6gz5JxncZJoX"
   },
   "source": [
    "По найденным полезностям и зная модель переходов легко найти оптимальную стратегию:\n",
    "$$\\pi^*(s) = argmax_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = argmax_a Q_i(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ml9AWeYrkNgf"
   },
   "source": [
    "### 1 балл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "7gd4m26TZOn3"
   },
   "outputs": [],
   "source": [
    "def get_optimal_action(\n",
    "    mdp, state_values, state, gamma=0.9\n",
    "):\n",
    "    \"\"\" Находим оптимальное действие, используя формулу выше. \"\"\"\n",
    "    if mdp.is_terminal(state): return None\n",
    "\n",
    "    actions = mdp.get_possible_actions(state)\n",
    "\n",
    "    i = -1\n",
    "    V_max = -np.inf\n",
    "    for j, action in enumerate(actions):\n",
    "        Q = get_action_value(mdp, state_values, state, action, gamma)\n",
    "        if V_max < Q:\n",
    "            V_max = Q\n",
    "            i = j\n",
    "\n",
    "    return actions[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "yv3MRqaQZSzs"
   },
   "outputs": [],
   "source": [
    "assert get_optimal_action(mdp, state_values, 's0', 0.9) == 'a1'\n",
    "assert get_optimal_action(mdp, state_values, 's1', 0.9) == 'a0'\n",
    "assert get_optimal_action(mdp, state_values, 's2', 0.9) == 'a1'\n",
    "\n",
    "assert get_optimal_action(mdp, {'s0': -1e10, 's1': 0, 's2': -2e10}, 's0', 0.9) == 'a0', \\\n",
    "    \"Убедитесь, что вы правильно обрабатываете отрицательные значения Q произвольной величины.\"\n",
    "assert get_optimal_action(mdp, {'s0': -2e10, 's1': 0, 's2': -1e10}, 's0', 0.9) == 'a1', \\\n",
    "    \"Убедитесь, что вы правильно обрабатываете отрицательные значения Q произвольной величины.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "V1KMZyhbZVVX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward:  0.4484\n"
     ]
    }
   ],
   "source": [
    "# Проверим среднее вознаграждение агента\n",
    "\n",
    "s = mdp.reset()\n",
    "rewards = []\n",
    "for _ in range(10000):\n",
    "    s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, 0.9))\n",
    "    rewards.append(r)\n",
    "\n",
    "print(\"average reward: \", np.mean(rewards))\n",
    "\n",
    "assert(0.40 < np.mean(rewards) < 0.55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xkokwmulZYYn"
   },
   "source": [
    "### Задание 2\n",
    "\n",
    "Теперь проверим работу итерации по ценностям на классической задаче FrozenLake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "E4V34IMzZbeH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*FFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "iter    0 | diff: 1.00000 | V(start): 0.000 \n",
      "iter    1 | diff: 0.90000 | V(start): 0.000 \n",
      "iter    2 | diff: 0.81000 | V(start): 0.000 \n",
      "iter    3 | diff: 0.72900 | V(start): 0.000 \n",
      "iter    4 | diff: 0.65610 | V(start): 0.000 \n",
      "iter    5 | diff: 0.59049 | V(start): 0.590 \n",
      "iter    6 | diff: 0.00000 | V(start): 0.590 \n",
      "Принято! Алгоритм сходится!\n"
     ]
    }
   ],
   "source": [
    "from mdp import FrozenLakeEnv\n",
    "mdp = FrozenLakeEnv(slip_chance=0)\n",
    "\n",
    "mdp.render()\n",
    "state_values = value_iteration(mdp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNPtPQo2ZdpV"
   },
   "source": [
    "Визуализируем нашу стратегию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "aQP4HnjNZg4C"
   },
   "outputs": [],
   "source": [
    "def draw_policy(mdp, state_values, gamma=0.9):\n",
    "    \"\"\"функция визуализации стратегии\"\"\"\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    h, w = mdp.desc.shape\n",
    "    states = sorted(mdp.get_all_states())\n",
    "    V = np.array([state_values[s] for s in states])\n",
    "    Pi = {\n",
    "        s: get_optimal_action(mdp, state_values, s, gamma)\n",
    "        for s in states\n",
    "    }\n",
    "    plt.imshow(\n",
    "        V.reshape(w, h),\n",
    "        cmap='gray', interpolation='none',\n",
    "        clim=(0, 1)\n",
    "    )\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(h) - .5)\n",
    "    ax.set_yticks(np.arange(w) - .5)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    Y, X = np.mgrid[0:4, 0:4]\n",
    "    a2uv = {'left': (-1, 0), 'down': (0, -1),\n",
    "            'right': (1, 0), 'up': (-1, 0)}\n",
    "    for y in range(h):\n",
    "        for x in range(w):\n",
    "            plt.text(x, y, str(mdp.desc[y, x].item()),\n",
    "                     color='g', size=12,\n",
    "                     verticalalignment='center',\n",
    "                     horizontalalignment='center',\n",
    "                     fontweight='bold')\n",
    "            a = Pi[y, x]\n",
    "            if a is None: continue\n",
    "            u, v = a2uv[a]\n",
    "            plt.arrow(x, y, u * .3, -v * .3,\n",
    "                      color='m', head_width=0.1,\n",
    "                      head_length=0.1)\n",
    "    plt.grid(color='b', lw=2, ls='-')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "UJ2zkkx2Zlec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after iteration 29\n",
      "iter    0 | diff: 0.00000 | V(start): 0.198 \n",
      "Принято! Алгоритм сходится!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "mdp = FrozenLakeEnv(map_name='8x8',slip_chance=0.1)\n",
    "state_values = {s : 0 for s in mdp.get_all_states()}\n",
    "\n",
    "for i in range(30):\n",
    "    clear_output(True)\n",
    "    print(\"after iteration %i\"%i)\n",
    "    state_values = value_iteration(mdp,\n",
    "                            state_values, num_iter=1)\n",
    "    draw_policy(mdp, state_values)\n",
    "    sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nltqOBDfZoFG"
   },
   "source": [
    "Посмотрим на оптимальную стратегию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "CKJ1oJapZq77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*FFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "right\n",
      "\n",
      "S*FFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "right\n",
      "\n",
      "SF*FFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "right\n",
      "\n",
      "SFFFFFFF\n",
      "FF*FFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "right\n",
      "\n",
      "SFFFFFFF\n",
      "FFF*FFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "right\n",
      "\n",
      "SFFFFFFF\n",
      "FFFF*FFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "right\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFF*FF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "right\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFF*F\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "down\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFF*F\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "down\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFH*F\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "right\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHF*\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "down\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFF*\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "down\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFH*\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "down\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFH*\n",
      "FFFHFFFG\n",
      "\n",
      "down\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFF*\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = mdp.reset()\n",
    "mdp.render()\n",
    "for t in range(100):\n",
    "    a = get_optimal_action(mdp, state_values, s, 0.9)\n",
    "    print(a, end='\\n\\n')\n",
    "    s, r, done, _ = mdp.step(a)\n",
    "    mdp.render()\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ksq-NonlZtHM"
   },
   "source": [
    "Тестируем на более сложном варианте окружения:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z6g4fbKkkZza"
   },
   "source": [
    "### 1 балл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "yOBqWNBfZv6v"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter    0 | diff: 0.80000 | V(start): 0.000 \n",
      "iter    1 | diff: 0.57600 | V(start): 0.000 \n",
      "iter    2 | diff: 0.41472 | V(start): 0.000 \n",
      "iter    3 | diff: 0.29860 | V(start): 0.000 \n",
      "iter    4 | diff: 0.24186 | V(start): 0.000 \n",
      "iter    5 | diff: 0.19349 | V(start): 0.000 \n",
      "iter    6 | diff: 0.15325 | V(start): 0.000 \n",
      "iter    7 | diff: 0.12288 | V(start): 0.000 \n",
      "iter    8 | diff: 0.09930 | V(start): 0.000 \n",
      "iter    9 | diff: 0.08037 | V(start): 0.000 \n",
      "iter   10 | diff: 0.06426 | V(start): 0.000 \n",
      "iter   11 | diff: 0.05129 | V(start): 0.000 \n",
      "iter   12 | diff: 0.04330 | V(start): 0.000 \n",
      "iter   13 | diff: 0.03802 | V(start): 0.033 \n",
      "iter   14 | diff: 0.03332 | V(start): 0.058 \n",
      "iter   15 | diff: 0.02910 | V(start): 0.087 \n",
      "iter   16 | diff: 0.01855 | V(start): 0.106 \n",
      "iter   17 | diff: 0.01403 | V(start): 0.120 \n",
      "iter   18 | diff: 0.00810 | V(start): 0.128 \n",
      "iter   19 | diff: 0.00555 | V(start): 0.133 \n",
      "iter   20 | diff: 0.00321 | V(start): 0.137 \n",
      "iter   21 | diff: 0.00247 | V(start): 0.138 \n",
      "iter   22 | diff: 0.00147 | V(start): 0.139 \n",
      "iter   23 | diff: 0.00104 | V(start): 0.140 \n",
      "iter   24 | diff: 0.00058 | V(start): 0.140 \n",
      "iter   25 | diff: 0.00036 | V(start): 0.141 \n",
      "iter   26 | diff: 0.00024 | V(start): 0.141 \n",
      "iter   27 | diff: 0.00018 | V(start): 0.141 \n",
      "iter   28 | diff: 0.00012 | V(start): 0.141 \n",
      "iter   29 | diff: 0.00007 | V(start): 0.141 \n",
      "iter   30 | diff: 0.00004 | V(start): 0.141 \n",
      "iter   31 | diff: 0.00003 | V(start): 0.141 \n",
      "iter   32 | diff: 0.00001 | V(start): 0.141 \n",
      "iter   33 | diff: 0.00001 | V(start): 0.141 \n",
      "Принято! Алгоритм сходится!\n",
      "Cреднее вознаграждение: 0.738\n",
      "Принято!\n"
     ]
    }
   ],
   "source": [
    "mdp = FrozenLakeEnv(slip_chance=0.2, map_name='8x8')\n",
    "state_values = value_iteration(mdp)\n",
    "\n",
    "total_rewards = []\n",
    "for game_i in range(1000):\n",
    "    s = mdp.reset()\n",
    "    rewards = []\n",
    "    for t in range(100):\n",
    "        a = get_optimal_action(mdp, state_values, s, 0.9)\n",
    "        s, r, done, _ = mdp.step(a)\n",
    "\n",
    "        rewards.append(r)\n",
    "        if done:\n",
    "            break\n",
    "    total_rewards.append(np.sum(rewards))\n",
    "\n",
    "print(\"Cреднее вознаграждение:\", np.mean(total_rewards))\n",
    "assert(0.6 <= np.mean(total_rewards) <= 0.8)\n",
    "print(\"Принято!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOEo-OheZyYP"
   },
   "source": [
    "### Задание 3\n",
    "\n",
    "Теперь рассмотрим алгоритм итерации по стратегиям (PI, policy iteration):\n",
    "\n",
    "---\n",
    "Initialize $\\pi_0$   `// случайно`\n",
    "\n",
    "For $n=0, 1, 2, \\dots$\n",
    "\n",
    "- Считаем функцию $V^{\\pi_{n}}$\n",
    "- Используя $V^{\\pi_{n}}$, считаем функцию $Q^{\\pi_{n}}$\n",
    "- Получаем новую стратегию: $\\pi_{n+1}(s) = \\operatorname*{argmax}_a Q^{\\pi_{n}}(s,a)$\n",
    "---\n",
    "\n",
    "PI включает в себя оценку полезности состояния, как внутренний шаг."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Diaeh1f7Z010"
   },
   "source": [
    "Вначале оценим полезности, используя текущую стратегию:\n",
    "$$V^{\\pi}(s) = \\sum_{s'} P(s,\\pi(s),s')[ R(s,\\pi(s),s') + \\gamma V^{\\pi}(s')]$$\n",
    "    Мы будем искать точное решение, хотя могли использовать и предыдущий итерационный подход. Для этого будем решать систему линейных уравнений относительно $V^{\\pi}(s_i)$ с помощью np.linalg.solve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3pFDjkE2kfsY"
   },
   "source": [
    "### 3 балла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "-RpV4Yw8Z3bi"
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import solve\n",
    "\n",
    "def compute_vpi(mdp, policy, gamma):\n",
    "    \"\"\"\n",
    "    Считем V^pi(s) для всех состояний, согласно стратегии.\n",
    "    :param policy: словарь состояние->действие {s : a}\n",
    "    :returns: словарь {state : V^pi(state)}\n",
    "    \"\"\"\n",
    "    states = mdp.get_all_states()\n",
    "    A, b = [], []\n",
    "    for i, state in enumerate(states):\n",
    "        if state in policy:\n",
    "            a = policy[state]\n",
    "\n",
    "            weighted_reward = 0\n",
    "            row = np.zeros(len(states))\n",
    "            row[i] = 1\n",
    "            for next_state, prob in mdp.get_next_states(state, a).items():\n",
    "                j = states.index(next_state)\n",
    "                row[j] -= gamma * prob\n",
    "                weighted_reward += prob * mdp.get_reward(state, a, next_state)\n",
    "            A.append(row)\n",
    "            b.append(weighted_reward)\n",
    "\n",
    "        else:\n",
    "            row = np.zeros(len(states))\n",
    "            row[i] = 1\n",
    "            A.append(row)\n",
    "            b.append(0)\n",
    "\n",
    "    A = np.array(A)\n",
    "    b = np.array(b)\n",
    "    values = solve(A, b)\n",
    "\n",
    "    state_values = {\n",
    "        states[i] : values[i]\n",
    "        for i in range(len(states))\n",
    "    }\n",
    "    return state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "qeb79E20Z6d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'s0': 8.031919916894896, 's1': 11.171970913211831, 's2': 8.924355463216552}\n"
     ]
    }
   ],
   "source": [
    "transition_probs = {\n",
    "    's0': {\n",
    "        'a0': {'s0': 0.5, 's2': 0.5},\n",
    "        'a1': {'s2': 1}\n",
    "    },\n",
    "    's1': {\n",
    "        'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "        'a1': {'s1': 0.95, 's2': 0.05}\n",
    "    },\n",
    "    's2': {\n",
    "        'a0': {'s0': 0.4, 's1': 0.6},\n",
    "        'a1': {'s0': 0.3, 's1': 0.3, 's2': 0.4}\n",
    "    }\n",
    "}\n",
    "rewards = {\n",
    "    's1': {'a0': {'s0': +5}},\n",
    "    's2': {'a1': {'s0': -1}}\n",
    "}\n",
    "mdp = MDP(transition_probs, rewards, initial_state='s0')\n",
    "\n",
    "gamma = 0.9\n",
    "\n",
    "test_policy = {\n",
    "    s: np.random.choice(mdp.get_possible_actions(s))\n",
    "    for s in mdp.get_all_states()}\n",
    "new_vpi = compute_vpi(mdp, test_policy, gamma)\n",
    "\n",
    "print(new_vpi)\n",
    "assert type(new_vpi) is dict, \\\n",
    "    \"функция compute_vpi должна возвращать словарь \\\n",
    "    {состояние s : V^pi(s) }\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Du2YNXpxZ9BT"
   },
   "source": [
    "Теперь обновляем стратегию на основе новых значений полезностей:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WsYlHPblkrSI"
   },
   "source": [
    "### 1 балл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "TGCHMeaUZ_Qj"
   },
   "outputs": [],
   "source": [
    "def compute_new_policy(mdp, vpi, gamma):\n",
    "    \"\"\"\n",
    "    Рассчитываем новую стратегию\n",
    "    :param vpi: словарь {state : V^pi(state) }\n",
    "    :returns: словарь {state : оптимальное действие}\n",
    "    \"\"\"\n",
    "    Q = {}\n",
    "    for state in mdp.get_all_states():\n",
    "        Q[state] = {}\n",
    "        for a in mdp.get_possible_actions(state):\n",
    "            values = []\n",
    "            for next_state in mdp.get_next_states(state, a):\n",
    "                r = mdp.get_reward(state, a, next_state)\n",
    "                p = mdp.get_transition_prob(\n",
    "                    state, a, next_state\n",
    "                )\n",
    "                values.append(p * (r + gamma * vpi[next_state]))\n",
    "\n",
    "            Q[state][a] = sum(values)\n",
    "\n",
    "    policy = {}\n",
    "    for state in mdp.get_all_states():\n",
    "        actions = mdp.get_possible_actions(state)\n",
    "        if actions:\n",
    "            Q_max = -np.inf\n",
    "            for action in actions:\n",
    "                if Q_max < Q[state][action]:\n",
    "                    Q_max = Q[state][action]\n",
    "                    policy[state] = action\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "1b1OXlg9aBsy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'s0': 'a1', 's1': 'a0', 's2': 'a0'}\n"
     ]
    }
   ],
   "source": [
    "new_policy = compute_new_policy(mdp, new_vpi, gamma)\n",
    "\n",
    "print(new_policy)\n",
    "\n",
    "assert type(new_policy) is dict, \\\n",
    "\"функция compute_new_policy должна возвращать словарь \\\n",
    "{состояние s: оптимальное действие}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15mJglOZaEmI"
   },
   "source": [
    "Собираем все в единый цикл:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIrgcIqKkxD2"
   },
   "source": [
    "### 1 балл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "2LcLHHhIaHAZ"
   },
   "outputs": [],
   "source": [
    "def policy_iteration(\n",
    "    mdp, policy=None, gamma = 0.9,\n",
    "    num_iter = 1000, min_difference = 1e-5\n",
    "):\n",
    "    \"\"\"\n",
    "    Запускаем цикл итерации по стратегиям\n",
    "    Если стратегия не определена, задаем случайную\n",
    "    \"\"\"\n",
    "    for i in range(num_iter):\n",
    "        if not policy:\n",
    "            policy = {}\n",
    "            for s in mdp.get_all_states():\n",
    "                if mdp.get_possible_actions(s):\n",
    "                    policy[s] = (\n",
    "                        np.random.choice(mdp.get_possible_actions(s))\n",
    "                    )\n",
    "\n",
    "        state_values = compute_vpi(mdp, policy, gamma)\n",
    "        policy = compute_new_policy(mdp, state_values, gamma)\n",
    "    return state_values, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddfLTSfgaJjU"
   },
   "source": [
    "Тестируем на FrozenLake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "4hLv3X0OaKmg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward:  0.877\n",
      "Принято!\n"
     ]
    }
   ],
   "source": [
    "mdp = FrozenLakeEnv(slip_chance=0.1)\n",
    "state_values, policy = policy_iteration(mdp)\n",
    "\n",
    "total_rewards = []\n",
    "for game_i in range(1000):\n",
    "    s = mdp.reset()\n",
    "    rewards = []\n",
    "    for t in range(100):\n",
    "        s, r, done, _ = mdp.step(policy[s])\n",
    "        rewards.append(r)\n",
    "        if done:\n",
    "            break\n",
    "    total_rewards.append(np.sum(rewards))\n",
    "\n",
    "print(\"average reward: \", np.mean(total_rewards))\n",
    "assert(0.8 <= np.mean(total_rewards) <= 0.95)\n",
    "print(\"Принято!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
